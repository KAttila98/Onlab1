{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broke-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model import TransformerModel\n",
    "import text_data\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-miller",
   "metadata": {},
   "source": [
    "# Data processing and model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "subject-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text_data.Corpus(\"data/wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "premium-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "married-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "seq_length = 35\n",
    "epochs = 1\n",
    "tr_batch_size = 20\n",
    "val_batch_size = 10\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amber-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = text_data.TextDataset(corpus.train, receptive_fields = 1, sample_size = seq_length)\n",
    "val_data = text_data.TextDataset(corpus.valid, receptive_fields = 1, sample_size = seq_length)\n",
    "test_data = text_data.TextDataset(corpus.test, receptive_fields = 1, sample_size = seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "august-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(tr_data, batch_size = tr_batch_size, shuffle = False)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size = val_batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = val_batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disabled-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(seq_length).to(device)\n",
    "    for batch, b in enumerate(train_loader):\n",
    "        \n",
    "        data, targets = b\n",
    "        \n",
    "        data = data.transpose(0,1).contiguous()\n",
    "        targets = targets.transpose(0,1).contiguous().view(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != seq_length:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        start = timeit.default_timer()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        stop = timeit.default_timer()\n",
    "        # print(f'\\\\Batch time: {stop-start}')\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_loader.dataset) // tr_batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(seq_length).to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch, b in enumerate(data_source):\n",
    "            data, targets = b\n",
    "            data = data.transpose(0,1).contiguous()\n",
    "            targets = targets.transpose(0,1).contiguous().view(-1)\n",
    "            if data.size(0) != seq_length:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source.dataset) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-channel",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "copyrighted-warrant",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kajud\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 44.32 | loss  8.97 | ppl  7841.95\n",
      "| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 42.90 | loss  7.67 | ppl  2151.15\n",
      "| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 43.39 | loss  7.42 | ppl  1661.47\n",
      "| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 42.96 | loss  7.01 | ppl  1106.95\n",
      "| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 42.93 | loss  6.83 | ppl   924.85\n",
      "| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 42.98 | loss  6.75 | ppl   855.23\n",
      "| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 42.97 | loss  6.61 | ppl   744.91\n",
      "| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 43.03 | loss  6.56 | ppl   704.91\n",
      "| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 43.36 | loss  6.55 | ppl   696.27\n",
      "| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 43.38 | loss  6.53 | ppl   683.97\n",
      "| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 46.80 | loss  6.47 | ppl   644.07\n",
      "| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 45.78 | loss  6.47 | ppl   643.60\n",
      "| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 45.87 | loss  6.49 | ppl   660.90\n",
      "| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 46.82 | loss  6.43 | ppl   619.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 136.68s | valid loss 23.71 | valid ppl 19734723694.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 4.51 | ms/batch 43.31 | loss  6.36 | ppl   578.91\n",
      "| epoch   2 |   400/ 2983 batches | lr 4.51 | ms/batch 43.38 | loss  6.28 | ppl   533.13\n",
      "| epoch   2 |   600/ 2983 batches | lr 4.51 | ms/batch 43.40 | loss  6.37 | ppl   583.93\n",
      "| epoch   2 |   800/ 2983 batches | lr 4.51 | ms/batch 43.45 | loss  6.14 | ppl   464.50\n",
      "| epoch   2 |  1000/ 2983 batches | lr 4.51 | ms/batch 43.02 | loss  6.10 | ppl   447.62\n",
      "| epoch   2 |  1200/ 2983 batches | lr 4.51 | ms/batch 43.10 | loss  6.12 | ppl   455.73\n",
      "| epoch   2 |  1400/ 2983 batches | lr 4.51 | ms/batch 43.45 | loss  6.04 | ppl   419.92\n",
      "| epoch   2 |  1600/ 2983 batches | lr 4.51 | ms/batch 44.00 | loss  6.02 | ppl   410.68\n",
      "| epoch   2 |  1800/ 2983 batches | lr 4.51 | ms/batch 44.10 | loss  6.06 | ppl   427.04\n",
      "| epoch   2 |  2000/ 2983 batches | lr 4.51 | ms/batch 44.04 | loss  6.10 | ppl   444.34\n",
      "| epoch   2 |  2200/ 2983 batches | lr 4.51 | ms/batch 43.67 | loss  6.03 | ppl   417.78\n",
      "| epoch   2 |  2400/ 2983 batches | lr 4.51 | ms/batch 43.03 | loss  6.06 | ppl   427.73\n",
      "| epoch   2 |  2600/ 2983 batches | lr 4.51 | ms/batch 42.99 | loss  6.10 | ppl   444.62\n",
      "| epoch   2 |  2800/ 2983 batches | lr 4.51 | ms/batch 43.02 | loss  6.05 | ppl   425.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 134.70s | valid loss 23.07 | valid ppl 10423269335.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 4.29 | ms/batch 43.35 | loss  6.04 | ppl   419.42\n",
      "| epoch   3 |   400/ 2983 batches | lr 4.29 | ms/batch 43.43 | loss  5.99 | ppl   398.31\n",
      "| epoch   3 |   600/ 2983 batches | lr 4.29 | ms/batch 43.31 | loss  6.07 | ppl   433.78\n",
      "| epoch   3 |   800/ 2983 batches | lr 4.29 | ms/batch 45.28 | loss  5.86 | ppl   350.72\n",
      "| epoch   3 |  1000/ 2983 batches | lr 4.29 | ms/batch 44.54 | loss  5.83 | ppl   341.26\n",
      "| epoch   3 |  1200/ 2983 batches | lr 4.29 | ms/batch 44.55 | loss  5.87 | ppl   352.95\n",
      "| epoch   3 |  1400/ 2983 batches | lr 4.29 | ms/batch 44.59 | loss  5.78 | ppl   325.33\n",
      "| epoch   3 |  1600/ 2983 batches | lr 4.29 | ms/batch 44.57 | loss  5.76 | ppl   318.71\n",
      "| epoch   3 |  1800/ 2983 batches | lr 4.29 | ms/batch 44.51 | loss  5.81 | ppl   334.44\n",
      "| epoch   3 |  2000/ 2983 batches | lr 4.29 | ms/batch 44.54 | loss  5.85 | ppl   346.88\n",
      "| epoch   3 |  2200/ 2983 batches | lr 4.29 | ms/batch 45.89 | loss  5.79 | ppl   328.45\n",
      "| epoch   3 |  2400/ 2983 batches | lr 4.29 | ms/batch 46.49 | loss  5.83 | ppl   340.54\n",
      "| epoch   3 |  2600/ 2983 batches | lr 4.29 | ms/batch 44.75 | loss  5.87 | ppl   354.80\n",
      "| epoch   3 |  2800/ 2983 batches | lr 4.29 | ms/batch 43.25 | loss  5.84 | ppl   343.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 137.71s | valid loss 22.34 | valid ppl 5052599628.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 4.07 | ms/batch 43.34 | loss  5.83 | ppl   339.85\n",
      "| epoch   4 |   400/ 2983 batches | lr 4.07 | ms/batch 43.36 | loss  5.79 | ppl   327.65\n",
      "| epoch   4 |   600/ 2983 batches | lr 4.07 | ms/batch 43.65 | loss  5.87 | ppl   354.62\n",
      "| epoch   4 |   800/ 2983 batches | lr 4.07 | ms/batch 43.64 | loss  5.68 | ppl   291.88\n",
      "| epoch   4 |  1000/ 2983 batches | lr 4.07 | ms/batch 43.23 | loss  5.64 | ppl   281.19\n",
      "| epoch   4 |  1200/ 2983 batches | lr 4.07 | ms/batch 43.57 | loss  5.67 | ppl   290.47\n",
      "| epoch   4 |  1400/ 2983 batches | lr 4.07 | ms/batch 43.54 | loss  5.59 | ppl   267.17\n",
      "| epoch   4 |  1600/ 2983 batches | lr 4.07 | ms/batch 43.25 | loss  5.57 | ppl   263.25\n",
      "| epoch   4 |  1800/ 2983 batches | lr 4.07 | ms/batch 43.69 | loss  5.62 | ppl   277.22\n",
      "| epoch   4 |  2000/ 2983 batches | lr 4.07 | ms/batch 43.25 | loss  5.65 | ppl   283.84\n",
      "| epoch   4 |  2200/ 2983 batches | lr 4.07 | ms/batch 43.69 | loss  5.60 | ppl   270.64\n",
      "| epoch   4 |  2400/ 2983 batches | lr 4.07 | ms/batch 43.90 | loss  5.66 | ppl   286.26\n",
      "| epoch   4 |  2600/ 2983 batches | lr 4.07 | ms/batch 43.79 | loss  5.70 | ppl   298.57\n",
      "| epoch   4 |  2800/ 2983 batches | lr 4.07 | ms/batch 43.16 | loss  5.67 | ppl   289.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 134.97s | valid loss 22.56 | valid ppl 6262416597.94\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 4 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-department",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "boring-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, b in enumerate(val_loader):\n",
    "    data, targets = b\n",
    "    data = data.transpose(0,1).contiguous()\n",
    "    targets = targets.transpose(0,1).contiguous().view(-1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "independent-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = corpus.dictionary.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "passing-runner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<eos> = Homarus gammarus = <eos> <eos> Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Input:\")\n",
    "' '.join([decoder[i] for i in data[:,0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "historic-livestock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'= Homarus gammarus = <eos> <eos> Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Target:\")\n",
    "' '.join([decoder[i] for i in targets.view(seq_length, val_batch_size)[:,0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adaptive-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "sentence = corpus.test[i*seq_length:(i+1)*seq_length].unsqueeze(0).cuda()\n",
    "generated = sentence.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "compliant-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with seed:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Generating text with seed:\")\n",
    "' '.join([decoder[i] for i in generated.transpose(0,1).tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cloudy-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "data = generated\n",
    "for i in range(30):\n",
    "    src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "    output = model(data, src_mask)\n",
    "    output_flat = torch.argmax(softmax(output.view(-1, ntokens)), dim = 1)\n",
    "    generated = torch.cat((generated, output_flat[-1].view(-1,1)), dim = 0)\n",
    "    data = generated[-seq_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "noble-latitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television starling 's common starling 's common starling 's common starling 's common starling 's common starling 's common starling 's common starling is a common starling 's common starling is\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Generated text:\")\n",
    "' '.join([decoder[i] for i in generated.transpose(0,1).tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-concentrate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

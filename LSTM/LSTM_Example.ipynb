{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rubber-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_data\n",
    "import wikitext_data\n",
    "from CustomLSTM import CustomLSTM\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-rabbit",
   "metadata": {},
   "source": [
    "# Data preprocessing and model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "digital-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "permanent-huntington",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kajud\\Documents\\GitHub\\Onlab1\\LSTM\\.data\\wikitext-2-v1.zip: 100%|█████████| 4.48M/4.48M [00:08<00:00, 518kB/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = wikitext_data.Corpus(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "separated-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(corpus.vocab.stoi)\n",
    "input_sz = 200\n",
    "hidden_sz = 128\n",
    "seq_length = 40\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legendary-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Embedding(n_tokens, input_sz),\n",
    "    CustomLSTM(input_sz = input_sz, hidden_sz = hidden_sz, return_states = False, return_sequences = False),\n",
    "    nn.Linear(hidden_sz, n_tokens)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorporated-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ready-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, targets):\n",
    "        \"\"\"\n",
    "        Train 1 time\n",
    "        :param inputs: Tensor[batch, timestep, channels]\n",
    "        :param targets: Torch tensor [batch, timestep, channels]\n",
    "        :return: float loss\n",
    "        \"\"\"\n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = criterion(logits.view(-1, n_tokens),\n",
    "                         targets.long().view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hundred-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = wikitext_data.TextDataset(corpus.train, in_out_overlap = False, input_size = seq_length, seq_len=seq_length + 1, stride = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "funded-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 256, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-natural",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "persistent-church",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8008/8008 [01:29<00:00, 89.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] loss: 5.847764015197754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(epochs):\n",
    "    for b in tqdm(train_loader):\n",
    "        inp, out = b\n",
    "        loss = train(inp, out)\n",
    "        \n",
    "    print(f'[{e + 1}/{epochs}] loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-appraisal",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "theoretical-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = corpus.test[0:40].unsqueeze(0).cuda()\n",
    "generated = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "early-controversy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with seed:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'= robert <unk> = robert <unk> is an english film , television and theatre actor . he had a guest @-@ starring role on the television series the bill in 2000 . this was followed by a starring role in'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Generating text with seed:\")\n",
    "' '.join([corpus.vocab.itos[i] for i in generated.tolist()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "weighted-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= robert <unk> = robert <unk> is an english film , television and theatre actor . he had a guest @-@ starring role on the television series the bill in 2000 . this was followed by a starring role in the <unk> <unk> , and the <unk> <unk> , and the <unk> <unk> , and the <unk> <unk> , and the <unk> <unk> , and the <unk> <unk> , and the <unk> <unk> , and the <unk> <unk> , and the <unk> <unk> , and the <unk> <unk> , and\n"
     ]
    }
   ],
   "source": [
    "sample_size = 40\n",
    "softmax = nn.Softmax(dim = -1)\n",
    "for i in range(50): # Generating 10 consecutive words\n",
    "    y_hats = model(sentence)\n",
    "    preds = torch.argmax(softmax(y_hats), dim = -1).unsqueeze(0)\n",
    "    generated = torch.cat((generated, preds), dim=1)\n",
    "    sentence = generated[:,-sample_size:]\n",
    "\n",
    "l_gen = generated.tolist()[0]\n",
    "gen_text = ' '.join([corpus.vocab.itos[i] for i in l_gen])\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-affiliate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
